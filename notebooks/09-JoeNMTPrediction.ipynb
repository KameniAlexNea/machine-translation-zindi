{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from joeynmt.config import load_config, parse_global_args\n",
    "from joeynmt.prediction import predict, prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model interface\n",
    "class JoeyNMTModel:\n",
    "    '''\n",
    "    JoeyNMTModel which load JoeyNMT model for inference.\n",
    "\n",
    "    :param config_path: Path to YAML config file\n",
    "    :param n_best: return this many hypotheses, <= beam (currently only 1)\n",
    "    '''\n",
    "    def __init__(self, config_path: str, n_best: int = 1):\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        cfg = load_config(config_path)\n",
    "        args = parse_global_args(cfg, rank=0, mode=\"translate\")\n",
    "        self.args = args._replace(test=args.test._replace(n_best=n_best))\n",
    "        # build model\n",
    "        self.model, _, _, self.test_data = prepare(self.args, rank=0, mode=\"translate\")\n",
    "\n",
    "    def _translate_data(self):\n",
    "        _, _, hypotheses, trg_tokens, trg_scores, _ = predict(\n",
    "            model=self.model,\n",
    "            data=self.test_data,\n",
    "            compute_loss=False,\n",
    "            device=self.args.device,\n",
    "            rank=0,\n",
    "            n_gpu=self.args.n_gpu,\n",
    "            normalization=\"none\",\n",
    "            num_workers=self.args.num_workers,\n",
    "            args=self.args.test,\n",
    "            autocast=self.args.autocast,\n",
    "        )\n",
    "        return hypotheses, trg_tokens, trg_scores\n",
    "\n",
    "    def translate(self, sentence) -> list:\n",
    "        '''\n",
    "        Translate the given sentence.\n",
    "\n",
    "        :param sentence: Sentence to be translated\n",
    "        :return:\n",
    "        - translations: (list of str) possible translations of the sentence.\n",
    "        '''\n",
    "        self.test_data.set_item(sentence.strip())\n",
    "        translations, _, _ = self._translate_data()\n",
    "        assert len(translations) == len(self.test_data) * self.args.test.n_best\n",
    "        self.test_data.reset_cache()\n",
    "        return translations\n",
    "\n",
    "# Load model\n",
    "config_path = \"models/dyu_fr/config.yaml\" # Change this to the path to your model congig file\n",
    "model = JoeyNMTModel(config_path=config_path, n_best=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate\n",
    "model.translate(sentence=\"i tɔgɔ bi cogodɔ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extra_dataset/chat_gpt/dataset\") as file:\n",
    "    data = file.readlines()\n",
    "data: list[str] = [\n",
    "    i.strip() for i in data if i.strip()\n",
    "]\n",
    "data_dyu = [\n",
    "\ti.replace(\"Dyula:\", \"\").strip() for i in data if \"Dyula:\" in i\n",
    "]\n",
    "data_fr = [\n",
    "\ti.replace(\"French:\", \"\").strip() for i in data if \"French:\" in i\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(index = 0):\n",
    "\tquery = data_dyu[index]\n",
    "\texpected = data_fr[index]\n",
    "\n",
    "\tpredicted = model.translate(query)\n",
    "\n",
    "\tprint(\"Expected   :\", expected)\n",
    "\tprint(\"Translated :\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in random.sample(range(len(data_dyu)), k=25):\n",
    "    print(\"Index\", i)\n",
    "    make_prediction(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
